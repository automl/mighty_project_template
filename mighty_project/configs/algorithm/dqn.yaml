# @package _global_
algorithm: DQN
q_func: ???

algorithm_kwargs:
  # Core architecture / model
  n_units: 256
  q_kwargs:
    dueling: False
    feature_extractor_kwargs:
      architecture: mlp
      n_layers: 1
      hidden_sizes: [256]
    head_kwargs:
      hidden_sizes: [256]

  # Exploration (decaying ε‐greedy)
  policy_class:
    _target_: mighty.mighty_exploration.DecayingEpsilonGreedy
  policy_kwargs:
    epsilon_start: 1.0
    epsilon_final: 0.04
    epsilon_decay_steps: 8000

  # Replay‐buffer settings
  replay_buffer_class:
    _target_: mighty.mighty_replay.MightyReplay
  replay_buffer_kwargs:
    capacity: 100000

  # Training hyperparameters
  learning_rate: 2.3e-3
  batch_size: 128
  gamma: 0.99
  learning_starts: 1000       # wait 1k transitions before training

  # Target‐network / updating (hard update every 1k ∇‐steps)
  use_target: True
  soft_update_weight: 0.005
  target_update_freq: null

  # Double DQN update
  td_update_class: mighty.mighty_update.QLearning

  td_update_kwargs:
    gamma: 0.99
    optimizer_class:
      _target_: torch.optim.Adam
    optimizer_kwargs:
      lr: 2.3e-3
      weight_decay: 1e-5
      eps: 1e-6
    max_grad_norm: 10.0

  # Checkpointing
  save_replay: False
  n_gradient_steps: 128