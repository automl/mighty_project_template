# @package _global_
algorithm: SAC

algorithm_kwargs:
  # network sizes (PPO-style)
  n_policy_units:     256       # will become hidden_sizes=[8,8]
  n_critic_units:     256       # same for both Q-nets
  soft_update_weight: 0.005    # maps to tau

  # Replay buffer
  replay_buffer_class:
    _target_: mighty.mighty_replay.MightyReplay
  replay_buffer_kwargs:
    capacity: 1e6

  # Scheduling & batch-updates
  batch_size:       256
  learning_starts:  10000
  update_every:     1
  n_gradient_steps: 1

  # Learning rates
  policy_lr: 3e-4
  q_lr:      3e-4

  # SAC hyperparameters
  gamma: 0.99
  alpha: 0.2
  auto_alpha: True
  target_entropy: null
  alpha_lr: 3e-4

  # Exploration wrapper
  policy_class: mighty.mighty_exploration.StochasticPolicy
  policy_kwargs:
    entropy_coefficient: 0.0
    discrete: False

  normalize_obs: True